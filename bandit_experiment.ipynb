{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45718ddbdacc17ac",
      "metadata": {
        "id": "45718ddbdacc17ac"
      },
      "source": [
        "# **Reinforcement Learning: Tarea 1, Estudio comparativo de algoritmos en un problema de k-armed bandit**\n",
        "\n",
        "## **Autores:** Ana Gil Molina, José María García Ortiz y Levi Malest Villarreal\n",
        "\n",
        "<br>\n",
        "\n",
        "*Description:* El experimento compara el rendimiento de algoritmos epsilon-greedy en un problema de k-armed bandit.\n",
        "Se generan distintas gráficas que explicarán las ventajas y desventajas de cada algoritmo a la hora de resolver este problema.\n",
        "\n",
        "<br>\n",
        "\n",
        "This software is licensed under the GNU General Public License v3.0 (GPL-3.0),\n",
        "with the additional restriction that it may not be used for commercial purposes.\n",
        "\n",
        "For more details about GPL-3.0: https://www.gnu.org/licenses/gpl-3.0.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c1697e197fa5a08",
      "metadata": {
        "id": "7c1697e197fa5a08"
      },
      "source": [
        "## **Preparación del entorno**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Copiar el repositorio ``JMGO-coding/k_brazos_GGM``.\n",
        "\n",
        "!git clone https://github.com/JMGO-coding/k_brazos_GGM.git\n",
        "!cd k_brazos_GGM/"
      ],
      "metadata": {
        "id": "q4_AzUhO4osU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81a771f0-39c1-489f-857a-687c44c2942e"
      },
      "id": "q4_AzUhO4osU",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'k_brazos_GGM'...\n",
            "remote: Enumerating objects: 192, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 192 (delta 20), reused 1 (delta 1), pack-reused 155 (from 1)\u001b[K\n",
            "Receiving objects: 100% (192/192), 380.20 KiB | 1.55 MiB/s, done.\n",
            "Resolving deltas: 100% (76/76), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4582eec6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-29T15:16:53.845102Z",
          "start_time": "2025-01-29T15:16:53.842529Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "4582eec6",
        "outputId": "aef54f2c-7870-494c-a8c7-c320e56c1d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python311.zip', '/usr/lib/python3.11', '/usr/lib/python3.11/lib-dynload', '', '/usr/local/lib/python3.11/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.11/dist-packages/IPython/extensions', '/usr/local/lib/python3.11/dist-packages/setuptools/_vendor', '/root/.ipython', '/content/k_brazos_GGM', '/content/k_brazos_GGM/src']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'plot_arm_statistics' from 'plotting' (/content/k_brazos_GGM/src/plotting/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e16078a5373f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malgorithms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpsilonGreedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0marms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArmNormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBandit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_average_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_optimal_selections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_arm_statistics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_regret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_arm_statistics' from 'plotting' (/content/k_brazos_GGM/src/plotting/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#@title Importamos todas las clases y funciones\n",
        "\n",
        "import sys\n",
        "\n",
        "# Añadir los directorio fuentes al path de Python\n",
        "sys.path.append('/content/k_brazos_GGM')\n",
        "sys.path.append('/content/k_brazos_GGM/src')\n",
        "\n",
        "\n",
        "\n",
        "# Verificar que se han añadido correctamente\n",
        "print(sys.path)\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List\n",
        "\n",
        "#from src.algorithms import Algorithm, EpsilonGreedy\n",
        "#from src.arms import ArmNormal, Bandit\n",
        "#from src.plotting import plot_average_rewards, plot_optimal_selections\n",
        "\n",
        "from algorithms import Algorithm, EpsilonGreedy\n",
        "from arms import ArmNormal, Bandit\n",
        "from plotting import plot_average_rewards, plot_optimal_selections, plot_arm_statistics, plot_regret\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e67de1a19a3698f",
      "metadata": {
        "id": "4e67de1a19a3698f"
      },
      "source": [
        "## **Experimento**\n",
        "\n",
        "Cada algoritmo se ejecuta en un problema de k-armed bandit durante un número de pasos de tiempo y ejecuciones determinado.\n",
        "Se comparan los resultados de los algoritmos en términos de recompensa promedio.\n",
        "\n",
        "Por ejemplo. Dado un bandido de k-brazos, se ejecutan dos algoritmos epsilon-greedy con diferentes valores de epsilon. Se estudia la evolución de cada política  en un número de pasos, por ejemplo, mil pasos. Entonces se repite el experimento un número de veces, por ejemplo, 500 veces. Es decir, se ejecutan 500 veces la evolución de cada algoritmo en 1000 pasos. Para cada paso calculamos el promedio de las recoponensas obtenidas en esas 500 veces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7377ca48ee0f5946",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-29T15:16:55.712927Z",
          "start_time": "2025-01-29T15:16:55.705949Z"
        },
        "id": "7377ca48ee0f5946"
      },
      "outputs": [],
      "source": [
        "def run_experiment(bandit: Bandit, algorithms: List[Algorithm], steps: int, runs: int):\n",
        "\n",
        "    optimal_arm = bandit.optimal_arm  # Necesario para calcular el porcentaje de selecciones óptimas.\n",
        "\n",
        "    rewards = np.zeros((len(algorithms), steps)) # Matriz para almacenar las recompensas promedio.\n",
        "\n",
        "    optimal_selections = np.zeros((len(algorithms), steps))  # Matriz para almacenar el porcentaje de selecciones óptimas.\n",
        "\n",
        "    np.random.seed(seed)  # Asegurar reproducibilidad de resultados.\n",
        "\n",
        "    for run in range(runs):\n",
        "        current_bandit = Bandit(arms=bandit.arms)\n",
        "\n",
        "        for algo in algorithms:\n",
        "            algo.reset() # Reiniciar los valores de los algoritmos.\n",
        "\n",
        "        total_rewards_per_algo = np.zeros(len(algorithms)) # Acumulador de recompensas por algoritmo. Necesario para calcular el promedio.\n",
        "\n",
        "        for step in range(steps):\n",
        "            for idx, algo in enumerate(algorithms):\n",
        "                chosen_arm = algo.select_arm() # Seleccionar un brazo según la política del algoritmo.\n",
        "                reward = current_bandit.pull_arm(chosen_arm) # Obtener la recompensa del brazo seleccionado.\n",
        "                algo.update(chosen_arm, reward) # Actualizar el valor estimado del brazo seleccionado.\n",
        "\n",
        "                rewards[idx, step] += reward # Acumular la recompensa obtenida en la matriz rewards para el algoritmo idx en el paso step.\n",
        "                total_rewards_per_algo[idx] += reward # Acumular la recompensa obtenida en total_rewards_per_algo para el algoritmo idx.\n",
        "\n",
        "                #TODO: modificar optimal_selections cuando el brazo elegido se corresponda con el brazo óptimo optimal_arm\n",
        "\n",
        "\n",
        "    rewards /= runs\n",
        "\n",
        "    # TODO: calcular el porcentaje de selecciones óptimas y almacenar en optimal_selections\n",
        "\n",
        "    return rewards, optimal_selections\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(bandit: Bandit, algorithms: List[Algorithm], steps: int, runs: int):\n",
        "    \"\"\"\n",
        "    Ejecuta experimentos comparativos entre diferentes algoritmos.\n",
        "\n",
        "    :param bandit: Instancia de Bandit configurada para el experimento.\n",
        "    :param algorithms: Lista de instancias de algoritmos a comparar.\n",
        "    :param steps: Número de pasos de tiempo por ejecución.\n",
        "    :param runs: Número de ejecuciones independientes.\n",
        "    :return: Tuple de tres elementos: recompensas promedio, porcentaje de selecciones óptimas, y estadísticas de brazos.\n",
        "    :rtype: Tuple of (np.ndarray, np.ndarray, list)\n",
        "    \"\"\"\n",
        "\n",
        "    k = bandit.k\n",
        "    optimal_arm = bandit.optimal_arm\n",
        "\n",
        "    # Inicializar matrices para recompensas y selecciones óptimas\n",
        "    rewards = np.zeros((len(algorithms), steps))\n",
        "    optimal_selections = np.zeros((len(algorithms), steps))\n",
        "\n",
        "    # Inicializar el regret global acumulado por cada algoritmo\n",
        "    global_cumulative_regret_per_algo = np.zeros(len(algorithms),steps)\n",
        "\n",
        "    for run in range(runs):\n",
        "        # Crear una nueva instancia del bandit para cada ejecución\n",
        "        current_bandit = Bandit(arms=bandit.arms)\n",
        "\n",
        "        # Obtener la recompensa esperada óptima\n",
        "        q_max = current_bandit.get_expected_value(current_bandit.optimal_arm)\n",
        "\n",
        "        for algo in algorithms:\n",
        "            algo.reset()\n",
        "\n",
        "        # Inicializar recompensas acumuladas por algoritmo para esta ejecución\n",
        "        total_rewards_per_algo = np.zeros(len(algorithms))  # Para análisis por rechazo\n",
        "\n",
        "        # Inicializar recompensas acumuladas por algoritmo para esta ejecución\n",
        "        # cumulative_rewards_per_algo = np.zeros(len(algorithms))\n",
        "\n",
        "        num_choices_list = [[0 for _ in range(len(current_bandit.arms))] for _ in algorithms]   # para almacenar un conteo de las selecciones de cada brazo del bandido para cada algoritmo\n",
        "\n",
        "        # Ejecutar cada algoritmo\n",
        "        for step in range(steps):\n",
        "            for idx, algo in enumerate(algorithms):\n",
        "                chosen_arm = algo.select_arm()\n",
        "                reward = current_bandit.pull_arm(chosen_arm)\n",
        "                algo.update(chosen_arm, reward)\n",
        "\n",
        "                num_choices_list[idx][chosen_arm] += 1   # Actualizamos el conteo del brazo seleccionado en la estapa correspondiente del algoritmo ``algo``\n",
        "\n",
        "                rewards[idx, step] += reward\n",
        "                total_rewards_per_algo[idx] += reward\n",
        "\n",
        "                global_cumulative_regret_per_algo[idx, step] = q_max*step - total_rewards_per_algo[idx]    # Actualziamos el regret acumulado en cada paso de decisión para cada algoritmo\n",
        "\n",
        "                if chosen_arm == optimal_arm:\n",
        "                    optimal_selections[idx, step] += 1\n",
        "\n",
        "    # Promediar las recompensas y el regret sobre todas las ejecuciones\n",
        "    rewards /= runs\n",
        "    optimal_selections = (optimal_selections / runs) * 100\n",
        "    proportional_cumulative_regret_per_algo = global_cumulative_regret_per_algo.copy()\n",
        "    proportional_cumulative_regret_per_algo /= runs\n",
        "\n",
        "    return rewards, optimal_selections, num_choices_list, proportional_cumulative_regret_per_algo"
      ],
      "metadata": {
        "id": "kUOlSxOa2Gbz"
      },
      "id": "kUOlSxOa2Gbz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3fb5453755a2b2a8",
      "metadata": {
        "id": "3fb5453755a2b2a8"
      },
      "source": [
        "### Ejecución del experimento\n",
        "\n",
        "Se realiza el experimento usando 10 brazos, cada uno de acuerdo a una distribución gaussina con desviación 1. Se realizan 500 ejecuciones de 1000 pasos cada una. Se contrastan 3 algoritmos epsilon greedy para valores epsilon: 0.0, 0.01, y 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "157bf5cc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-29T15:17:02.644124Z",
          "start_time": "2025-01-29T15:16:59.172459Z"
        },
        "id": "157bf5cc"
      },
      "outputs": [],
      "source": [
        "# Parámetros del experimento\n",
        "seed = 42\n",
        "np.random.seed(seed)  # Fijar la semilla para reproducibilidad\n",
        "\n",
        "k = 10        # Número de brazos\n",
        "steps = 1000  # Número de pasos que se ejecutarán cada algoritmo\n",
        "runs = 500    # Número de ejecuciones\n",
        "\n",
        "# Creación del bandit\n",
        "bandit = Bandit(arms=ArmNormal.generate_arms(k))    # Generar un bandido con k brazos de distribución normal\n",
        "print(bandit)\n",
        "\n",
        "optimal_arm = bandit.optimal_arm\n",
        "print(f\"Optimal arm: {optimal_arm + 1} with expected reward={bandit.get_expected_value(optimal_arm)}\")\n",
        "\n",
        "# Definir los algoritmos a comparar. En este caso son 3 algoritmos epsilon-greedy con diferentes valores de epsilon.\n",
        "algorithms = [EpsilonGreedy(k=k, epsilon=0), EpsilonGreedy(k=k, epsilon=0.01), EpsilonGreedy(k=k, epsilon=0.1)]\n",
        "\n",
        "# Extras\n",
        "optimal_arms_list = [optimal_arm for _ in range(len(algoritms))]\n",
        "stats = {}\n",
        "arm_stats = [stats for _ in range(len(algorithms))]\n",
        "for arm in bandit.arms:\n",
        "  stats[arm] = {\"media\":arm.mu , \"std\":arm.sigma}\n",
        "\n",
        "# Ejecutar el experimento y obtener las recompensas promedio y promedio de las selecciones óptimas\n",
        "rewards, optimal_selections, num_choices_list, proportional_cumulative_regret_per_algo = run_experiment(bandit, algorithms, steps, runs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "573c9612c3b14efb",
      "metadata": {
        "id": "573c9612c3b14efb"
      },
      "source": [
        "### Visualización de los resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3fd68ee",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-01-29T15:17:07.750227Z",
          "start_time": "2025-01-29T15:17:07.603719Z"
        },
        "id": "d3fd68ee"
      },
      "outputs": [],
      "source": [
        "plot_average_rewards(steps, rewards, algorithms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29fcd95b7fca79de",
      "metadata": {
        "id": "29fcd95b7fca79de"
      },
      "source": [
        "### Análisis detallado de la imagen\n",
        "\n",
        "La imagen muestra un gráfico de líneas titulado **\"Recompensa Promedio vs Pasos de Tiempo\"**, donde se analiza el desempeño de diferentes estrategias del algoritmo **ε-Greedy** en un entorno de multi-armed bandit. En el eje **x** se representan los **pasos de tiempo**, mientras que en el eje **y** se muestra la **recompensa promedio** obtenida por cada algoritmo.\n",
        "\n",
        "\n",
        "1. **Tres líneas de colores distintos representan diferentes valores de ε en el algoritmo ε-Greedy:**\n",
        "   - **Azul (ε = 0):** Representa una estrategia completamente **explotadora**, es decir, que siempre elige la acción que ha dado la mejor recompensa hasta ahora sin explorar nuevas opciones.\n",
        "   - **Naranja (ε = 0.01):** Representa una estrategia con una pequeña probabilidad del 1% de elegir una acción aleatoria (exploración).\n",
        "   - **Verde (ε = 0.1):** Representa una estrategia con un 10% de probabilidad de explorar acciones aleatorias.\n",
        "\n",
        "2. **Crecimiento de la recompensa promedio:**\n",
        "   - La línea **verde (ε=0.1)** alcanza rápidamente una recompensa promedio alta, lo que indica que la estrategia con mayor exploración aprende más rápido qué brazos del bandit son óptimos.\n",
        "   - La línea **naranja (ε=0.01)** también muestra un crecimiento, pero más lento en comparación con ε=0.1.\n",
        "   - La línea **azul (ε=0)** se mantiene en un nivel bajo de recompensa, lo que sugiere que no logra encontrar el mejor brazo porque no explora nuevas opciones.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4207366e56a23449",
      "metadata": {
        "id": "4207366e56a23449"
      },
      "source": [
        "## **Conclusiones**\n",
        "\n",
        "Hemos estudiado un  **experimento de toma de decisiones secuenciales**, modelado con un **Multi-Armed Bandit (MAB)**. Este problema es fundamental en el aprendizaje por refuerzo y la teoría de decisiones. La idea principal es que un agente debe aprender cuál es la mejor acción (brazo del bandit) a partir de la experiencia acumulada. Para este estudio nos hemos centrado solo en el estudio del algoritmo epsilon-greedy, llegando a las siguientes conclusiones a partir de los resultados obtenidos y la gráfica generada:\n",
        "\n",
        "#### **1. Exploración vs Explotación**\n",
        "El algoritmo **ε-Greedy** equilibra la exploración y la explotación:\n",
        "- **Explotación (ε=0)**: Siempre elige la mejor opción conocida, pero si inicialmente se selecciona un brazo subóptimo, nunca descubrirá otras opciones más rentables.\n",
        "- **Exploración (ε>0)**: Introduce aleatoriedad en la selección de acciones para descubrir nuevas opciones potencialmente mejores.\n",
        "\n",
        "El gráfico confirma este comportamiento:\n",
        "- **ε=0.1 (verde)** obtiene la mejor recompensa promedio a lo largo del tiempo porque explora lo suficiente como para encontrar rápidamente el mejor brazo.\n",
        "- **ε=0.01 (naranja)** explora menos, por lo que tarda más en converger a una recompensa alta.\n",
        "- **ε=0 (azul)** no explora en absoluto y queda atrapado en una recompensa subóptima.\n",
        "\n",
        "#### **2. Convergencia de los algoritmos**\n",
        "Los algoritmos con mayor exploración (ε=0.1) alcanzan una recompensa alta más rápido. Esto se debe a que:\n",
        "- Al principio, el algoritmo **no tiene información suficiente** sobre cuál es el mejor brazo.\n",
        "- Con el tiempo, al realizar exploraciones, descubre cuál es el mejor brazo y empieza a explotarlo más.\n",
        "- Un **balance entre exploración y explotación** es clave para maximizar la recompensa a largo plazo.\n",
        "\n",
        "\n",
        "#### **3. Aplicaciones y conclusiones**\n",
        "- En problemas de toma de decisiones **(ejemplo: recomendaciones, optimización de anuncios, medicina personalizada)**, una estrategia de exploración moderada como **ε=0.1** es más efectiva para encontrar la mejor opción rápidamente.\n",
        "- **La falta de exploración (ε=0)** lleva a un desempeño deficiente, ya que el agente puede quedarse atrapado en una elección subóptima.\n",
        "\n",
        "En conclusión, **el gráfico muestra cómo un nivel adecuado de exploración mejora significativamente el rendimiento del algoritmo en un entorno de aprendizaje por refuerzo**. 🚀"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_optimal_selections(steps, optimal_selections, algorithms)"
      ],
      "metadata": {
        "id": "KjG096D822DS"
      },
      "id": "KjG096D822DS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_arm_statistics(arm_stats, algorithms, optimal_arms_list, num_choices_list)"
      ],
      "metadata": {
        "id": "qOm051lC23FD"
      },
      "id": "qOm051lC23FD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_regret(steps, regret_accumulated=proportional_cumulative_regret_per_algo, algorithms)"
      ],
      "metadata": {
        "id": "-AnABKcK4o-i"
      },
      "id": "-AnABKcK4o-i",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}